---
layout: post
title: No CVE for this. It has never been in oficial kernel
date: 2023-06-28
categories: ["exploiting", "Linux", "Kernel"]
thumbnail: "assets/images/thumb1.png"
---

# No CVE for [this](https://lkml.org/lkml/2019/12/5/814) bug wich has never been in oficial kernel

## Introduction

In this blog post I will show how I wrote an exploit for [this](https://lkml.org/lkml/2019/12/5/814) bug wich has never been in oficial kernel, thus, to achieve local privilege escalation in Linux kernel 5.10.77.

**ptrace** is a system call wich you can take the control from another process. In this article, the PTRACE_GETFD command is introduced to Linux kernel. Who can get the file descriptor indicated from the vitcim process to trace it.

## The bug

```C
1 static int ptrace_getfd(struct task_struct *child, unsigned long fd)
2 {
3	struct files_struct *files;
4	struct file *file;
5	int ret = 0;
6
7	files = get_files_struct(child);
8	if (!files)
9		return -ENOENT;
10
11	spin_lock(&files->file_lock);
12	file = fcheck_files(files, fd);
13	if (!file)
14		ret = -EBADF;
15	else
16		get_file(file); // increment f_count
17	spin_unlock(&files->file_lock);
18	put_files_struct(files);
19
20	if (ret)
21		goto out;
22
23	ret = get_unused_fd_flags(0);
24	if (ret >= 0)
25		fd_install(ret, file); // install the reference in file table
26
27	fput(file); // decrement f_count
28 out:
29	return ret;
30 }
``` 

In line 16, get_file increments the f_count, well, it must do it, but in line 27 fput is decrementing again, wich it should not do it. The result is a one extra reference installed in the file table and f_count is the same. Two references with f_count = 1... If do close over one fd from them, we have an UAF over the other fd reference.


## Cache isolated

In this kernel, the file cache is isolated by the flag SLAB_ACCOUNT

```C
380 void __init files_init(void)
381 {
382	filp_cachep = kmem_cache_create("filp", sizeof(struct file), 0,
383			SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
384	percpu_counter_init(&nr_files, 0, GFP_KERNEL);
385 }
```

This mean that we can´t get other object from general purpose (kmalloc) to allocate over the UAF. How solve this? [Cross cache attack](https://veritas501.github.io/2023_03_07-Cross%20Cache%20Attack%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82%E5%88%86%E6%9E%90/) is a teqnique where you can free the file object created and return to budy allocator. After this, we can allocate other object from general purpose to exploit the bug.


## Cross Cache attack

For freeing one object from their slab, we must free the whole page of the object. Thus, we need to do a serie of calls to arrive to discard_slab(). When kmem_cache_free is called a serie of calls are done.

```
kmem_cache_free()
 |
 + slab_free() 
    | 
    + do_slab_free() 
       |
       +__slab_free()
           |
           + put_cpu_partial()
              |
              + unfreeze_partials()
                 |
                 + discard_slab()
```

In [kmem_cache_free](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L3160) the function only checks if the object is from their own cache, but the object will be freed. do_slab_free is the wrapper of [slab_free](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L3141). In [do_slab_free](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L3122) the function checks if the page wich is going to be freed is from the actual cpu partial active, if this is true, it will adds to the freelist, it calls to __slab_free otherwise.

Observe this image and these structs before to go on, assuming you are familiar with how the heap is organized in Linux kernel (in this case, SLUB in Ubuntu) and you know that there are types of objects with their own dedicated cache. One object from dedicated cache (for example file object "filp") can´t go to other dedicated cache (for example "jar_creds") or general purpose cache (kmalloc). In a full list, all the slab are with allocated objects. In a partial list, there are a mix with allocated and free objects. In a empty list, all the objects are freed.

![SLUB](/assets/images/SLUB.png)

```C
struct kmem_cache_node {
	spinlock_t list_lock;

#ifdef CONFIG_SLAB
	struct list_head slabs_partial;	/* partial list first, better asm code */
	struct list_head slabs_full;
	struct list_head slabs_free;
	unsigned long total_slabs;	/* length of all slab lists */
	unsigned long free_slabs;	/* length of free slab list only */
	unsigned long free_objects;
	unsigned int free_limit;
	unsigned int colour_next;	/* Per-node cache coloring */
	struct array_cache *shared;	/* shared per node */
	struct alien_cache **alien;	/* on other nodes */
	unsigned long next_reap;	/* updated without locking */
	int free_touched;		/* updated without locking */
#endif

#ifdef CONFIG_SLUB
	unsigned long nr_partial;
	struct list_head partial;
#ifdef CONFIG_SLUB_DEBUG
	atomic_long_t nr_slabs;
	atomic_long_t total_objects;
	struct list_head full;
#endif
#endif
};
```
```C
struct kmem_cache {
	struct kmem_cache_cpu __percpu *cpu_slab;
	/* Used for retrieving partial slabs, etc. */
	slab_flags_t flags;
	unsigned long min_partial;
	unsigned int size;	/* The size of an object including metadata */
	unsigned int object_size;/* The size of an object without metadata */
	struct reciprocal_value reciprocal_size;
	unsigned int offset;	/* Free pointer offset */
#ifdef CONFIG_SLUB_CPU_PARTIAL
	/* Number of per cpu partial objects to keep around */
	unsigned int cpu_partial;
#endif
...
struct kmem_cache_node *node[MAX_NUMNODES];
};
```
```C
struct kmem_cache_cpu {
	void **freelist;	/* Pointer to next available object */
	unsigned long tid;	/* Globally unique transaction id */
	struct page *page;	/* The slab from which we are allocating */
#ifdef CONFIG_SLUB_CPU_PARTIAL
	struct page *partial;	/* Partially allocated frozen slabs */
#endif
#ifdef CONFIG_SLUB_STATS
	unsigned stat[NR_SLUB_STAT_ITEMS];
#endif
};
```

###### [__slab_free](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L2966)

The kernel will check if the page of the object freed is from active slab or on a partial list. If this condition is met, the object will be freed, the function [put_cpu_partial](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L2377) is called otherwise. Going over, if the object is in a full slab and it is not in active slab, it will call to put_cpu_partial because we need to put the slab in the partial list to be handled. The full slabs are not accounted for. The next step into the function [put_cpu_partial](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L2418) the kernel checks if the number of the pobjects (slabs) in the partial list exceeds the limit, wich in this kernel are 13, and think if we are freeing one object from each 14 full slab, no actives, there will be 14 slab to be handled, and the last slab will cause a overflow in the partial list and the slab will be transfered to the node partial list, because of this, the kernel will call to [unfreeze_partials](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L2309). Into the function unfreeze_partials the pages (slabs) not empty will be transfered to the node partial list, [discard_slab](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L2347) will be called if the page (slab) is empty and not used (new.inuse = 0). See the image offered by this nice [write up](https://ruia-ruia.github.io/2022/08/05/CVE-2022-29582-io-uring/) explaining this.

![Control flow](/assets/images/freeslab.jpg)


Then, the plan is the following scenario, after gathering information:

```
$ sudo cat /sys/kernel/slab/filp/object_size
256
$ sudo cat /sys/kernel/slab/filp/objs_per_slab
16
$ sudo cat /sys/kernel/slab/filp/cpu_partial
13
```


* Alloc (cpu_partial + 1) * objs_per_slab = (13 + 1) * 16
* Alloc objs_per_slab - 1 = 15
* Alloc the vulnerable object
* Alloc objs_per_slab + 1 = 17
* Get UAF over the vulnerable object 
* Free the whole slab of the vulnerable object
* Free only one object from 14 full slab to cause overflow in the partial list


But, there is a problem :( in this kernel, there isn´t a whole slab with 16 objects in the same page until the slab allocated ~92..


| Slab | 0 |
|------|------|
| stdin | stdout | 
| stderr | object page A |
| object page A | object page A |
| object page B | object page B |
| object page C | object page C |
| object page C | object page A |
| object page A | object page D |
| object page D | object page D |
|------|------|

| Slab | ~92 |
|------|------|
| object page N | object page N | 
| object page N | object page N |
| object page N | object page N |
| object page N | object page N |
| object page N | object page N |
| object page N | object page N |
| object page N | object page N |
| object page N | object page N |
|------|------|

Thus, the plan is to use the same scenario but with cpu_partial = 128 (to ensure our success). Remember that the full slabs are not accounted for.


## msg_msg objects spray


